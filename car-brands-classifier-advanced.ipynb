{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization,Rescaling\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from efficientnet.tfkeras import EfficientNetB3\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#use if using gpu\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*4)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "# Set the path to your dataset\n",
    "train_data_dir = os.path.join(os.getcwd(),'dataset/train')\n",
    "test_data_dir = os.path.join(os.getcwd(),'dataset/test')\n",
    "# Define parameters\n",
    "img_width, img_height = 300, 300  # EfficientNet input size\n",
    "input_shape = (img_width, img_height, 3)\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "train_generator=tf.keras.utils.image_dataset_from_directory(train_data_dir,validation_split=0.2,\n",
    "                                                            seed=123,subset=\"training\",\n",
    "                                                            image_size=(img_width, img_height),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            label_mode='categorical'\n",
    "                                                            )\n",
    "\n",
    "val_generator=tf.keras.utils.image_dataset_from_directory(train_data_dir,\n",
    "                                                          validation_split=0.2,\n",
    "                                                          seed=123,subset=\"validation\",\n",
    "                                                          image_size=(img_width, img_height),\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          label_mode='categorical')\n",
    "\n",
    "# Use EfficientNetB3 as a base model\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False)\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model on top of the pre-trained base model\n",
    "model = Sequential()\n",
    "model.add(Rescaling(1./255,input_shape=input_shape))\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))  # Assuming you have 7 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning rate schedule\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[reduce_lr, early_stop]\n",
    "    )\n",
    "\n",
    "# Save the model\n",
    "model.save('car_brand_classifier_advanced_model.h5')\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "model=load_model('car_brand_classifier_advanced_model.h5')\n",
    "imsize=(300,300)\n",
    "labels=['Audi', 'Hyundai Creta', 'Mahindra Scorpio', 'Rolls Royce', 'Swift', 'Tata Safari', 'Toyota Innova']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_label=[]\n",
    "true_label=[]\n",
    "for i in sorted(os.listdir('dataset/test')):\n",
    "    for j in os.listdir(os.path.join('dataset/test',i)):\n",
    "        filename=os.path.join('dataset/test/',i,j)\n",
    "        img=image.load_img(filename, target_size=imsize)\n",
    "        resized_img=image.img_to_array(img)\n",
    "        final=np.expand_dims(resized_img,axis=0)\n",
    "        predictions=model.predict(final)\n",
    "        true_label.append(sorted(os.listdir('dataset/test')).index(i))\n",
    "        predicted_label.append(list(predictions[0]).index(max(predictions[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(true_label, predicted_label)\n",
    "cm_percentages = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_percentages, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels,\n",
    "            yticklabels=labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for 7 Classes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
